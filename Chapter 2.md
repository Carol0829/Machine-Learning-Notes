# 第二章 模型评估与选择

## 2.1 经验误差与过拟合

本节需要明确的几个在机器学习中的重要概念：

* 误差：学习器的实际预测输出与样本的真实输出之间的差异
  * 训练误差/经验误差：学习器在训练集上的误差
  * 泛化误差：用训练好的学习器在新样本上预测的误差

* 机器学习实际希望实现的，是减小泛化误差

* 过拟合与欠拟合
  * 欠拟合（underfitting）：对训练集的学习效果太差，一般是由于训练模型太简单导致的
  * 过拟合（overfitting）：在训练集上的训练效果过于好了，导致模型的泛化能力不强，泛化误差较大，一般是由于模型太过复杂导致的
  * 过拟合是在机器学习的实际训练中经常出现、较难解决的重要问题

## 2.2 评估方法

由于模型的泛化误差，也就是新样本的真实值不可得，所以需要从训练集中拆分出一部分“测试集”，将在这部分数据集中的“测试误差”作为泛化误差的近似。

介绍了拆分测试集的三种方法：

1. 留出法（hold-out）
  * 直接将数据集划分为两个互斥的集合，就是train_test_split函数的做法，随机分层抽样
  * 由于划分的训练集和测试集样本不同，对应的结果也会不同，单次留出法的结果不具有稳健性，需要采用多次留出法取平均值
  * 问题：可能会由于划分比例不均（训练集占比过大或过小）而带来方差或者偏差过大的问题（需要进行bias和variance的trade-off）
  
2. 交叉验证法（cross validation）
  * k折交叉验证，是实际比赛中很常用的方法
  * 每次抽取1/k作为测试集，其余的部分作为训练集，重复训练k次
  * 留一法：每次抽取1个样本作为测试集（k=数据集大小）
  
3. 自助法（bootstrapping） 
  * 有放回的随机采样
  * 样本始终不会被采到的概率：0.368

## 2.3 性能度量

* 回归问题中最常用的性能度量就是均方误差（MSE）

* 在分类问题中，有不同的性能度量方法：
  * 错误率与精度

  * 查准率、查全率与F1
    * 查准率：预测结果为正的样本中，真正为正样本（即预测正确）的占比 
    * 查全率：真实label为正的样本中，预测为正样本的占比
    * F1：由于查准率和查全率一般是相矛盾的，需要综合考虑，引入了F1 = 2 * 查准率 * 查全率 / 查准率 + 查全率
  
  * ROC与AUC
  
  * 代价敏感错误率与代价曲线：不同类型错误的损失不是相等的（增加了权重）

## 2.4 比较检验

一些概率论的东西，没细看

## 2.5 偏差和方差

对算法的期望预测误差（泛化误差）可以进行分解，分解成三项：

* 不可约的误差项（噪声）：无法控制的
* 偏差项（bias）：实际的期望值与预测的期望值之差的平方
* 方差项（variance）：平均方差

偏差项和方差项相加构成了均方误差，偏差和方差是相互矛盾的：当模型较为简单时，模型偏差较大、方差较小，容易欠拟合；当模型较为复杂时，模型偏差较小、方差较大，容易过拟合。

因此，在模型选择中，需要注意偏差和方差之间的权衡。

